# CMAKE generated file: DO NOT EDIT!
# Generated by "Ninja" Generator, CMake Version 3.30

# This file contains all the build statements describing the
# compilation DAG.

# =============================================================================
# Write statements declared in CMakeLists.txt:
# 
# Which is the root file.
# =============================================================================

# =============================================================================
# Project: engine
# Configurations: Debug
# =============================================================================

#############################################
# Minimal version of Ninja required by this file

ninja_required_version = 1.5


#############################################
# Set configuration variable for custom commands.

CONFIGURATION = Debug
# =============================================================================
# Include auxiliary files.


#############################################
# Include rules file.

include CMakeFiles/rules.ninja

# =============================================================================

#############################################
# Logical path to working directory; prefix for absolute paths.

cmake_ninja_workdir = C$:/Users/source/ai$ engine/engine/cmake-build-debug-visual-studio/

#############################################
# Utility command for edit_cache

build CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -E echo "No interactive CMake dialog available.""
  DESC = No interactive CMake dialog available...
  restat = 1

build edit_cache: phony CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" --regenerate-during-build -S"C:\Users\source\ai engine\engine" -B"C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio""
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build rebuild_cache: phony CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build list_install_components: phony


#############################################
# Utility command for install

build CMakeFiles/install.util: CUSTOM_COMMAND all
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -P cmake_install.cmake"
  DESC = Install the project...
  pool = console
  restat = 1

build install: phony CMakeFiles/install.util


#############################################
# Utility command for install/local

build CMakeFiles/install/local.util: CUSTOM_COMMAND all
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake"
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build install/local: phony CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build CMakeFiles/install/strip.util: CUSTOM_COMMAND all
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake"
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build install/strip: phony CMakeFiles/install/strip.util

# =============================================================================
# Write statements declared in CMakeLists.txt:
# C:/Users/source/ai engine/engine/CMakeLists.txt
# =============================================================================

# =============================================================================
# Object build statements for STATIC_LIBRARY target InferenceEngine


#############################################
# Order-only phony target for InferenceEngine

build cmake_object_order_depends_target_InferenceEngine: phony || .

build InferenceEngine/CMakeFiles/InferenceEngine.dir/library.cpp.obj.ddi: CXX_SCAN__InferenceEngine_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/library.cpp || cmake_object_order_depends_target_InferenceEngine
  DEP_FILE = InferenceEngine\CMakeFiles\InferenceEngine.dir\library.cpp.obj.ddi.d
  DYNDEP_INTERMEDIATE_FILE = InferenceEngine\CMakeFiles\InferenceEngine.dir\library.cpp.obj.ddi
  FLAGS = -O0 -std=gnu++20 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview
  OBJ_FILE = InferenceEngine\CMakeFiles\InferenceEngine.dir\library.cpp.obj
  PREPROCESSED_OUTPUT_FILE = InferenceEngine\CMakeFiles\InferenceEngine.dir\library.cpp.obj.ddi.i

build InferenceEngine/CMakeFiles/InferenceEngine.dir/library.cpp.obj: CXX_COMPILER__InferenceEngine_scanned_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/library.cpp | InferenceEngine/CMakeFiles/InferenceEngine.dir/library.cpp.obj.modmap || cmake_object_order_depends_target_InferenceEngine InferenceEngine/CMakeFiles/InferenceEngine.dir/CXX.dd
  DEP_FILE = InferenceEngine\CMakeFiles\InferenceEngine.dir\library.cpp.obj.d
  DYNDEP_MODULE_MAP_FILE = InferenceEngine\CMakeFiles\InferenceEngine.dir\library.cpp.obj.modmap
  FLAGS = -O0 -std=gnu++20 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview
  OBJECT_DIR = InferenceEngine\CMakeFiles\InferenceEngine.dir
  OBJECT_FILE_DIR = InferenceEngine\CMakeFiles\InferenceEngine.dir
  TARGET_COMPILE_PDB = InferenceEngine\CMakeFiles\InferenceEngine.dir\InferenceEngine.pdb
  TARGET_PDB = InferenceEngine\InferenceEngine.pdb
  dyndep = InferenceEngine/CMakeFiles/InferenceEngine.dir/CXX.dd

build InferenceEngine/CMakeFiles/InferenceEngine.dir/CXX.dd | C$:/Users/source/ai$ engine/engine/cmake-build-debug-visual-studio/InferenceEngine/CMakeFiles/InferenceEngine.dir/CXXModules.json InferenceEngine/CMakeFiles/InferenceEngine.dir/library.cpp.obj.modmap: CXX_DYNDEP__InferenceEngine_Debug InferenceEngine/CMakeFiles/InferenceEngine.dir/library.cpp.obj.ddi | C$:/Users/source/ai$ engine/engine/cmake-build-debug-visual-studio/InferenceEngine/CMakeFiles/InferenceEngine.dir/CXXDependInfo.json


# =============================================================================
# Link build statements for STATIC_LIBRARY target InferenceEngine


#############################################
# Link the static library InferenceEngine\InferenceEngine.lib

build InferenceEngine/InferenceEngine.lib: CXX_STATIC_LIBRARY_LINKER__InferenceEngine_Debug InferenceEngine/CMakeFiles/InferenceEngine.dir/library.cpp.obj
  LANGUAGE_COMPILE_FLAGS = -O0 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview
  OBJECT_DIR = InferenceEngine\CMakeFiles\InferenceEngine.dir
  POST_BUILD = cd .
  PRE_LINK = cd .
  TARGET_COMPILE_PDB = InferenceEngine\CMakeFiles\InferenceEngine.dir\InferenceEngine.pdb
  TARGET_FILE = InferenceEngine\InferenceEngine.lib
  TARGET_PDB = InferenceEngine\InferenceEngine.pdb


#############################################
# Utility command for edit_cache

build InferenceEngine/CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -E echo "No interactive CMake dialog available.""
  DESC = No interactive CMake dialog available...
  restat = 1

build InferenceEngine/edit_cache: phony InferenceEngine/CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build InferenceEngine/CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" --regenerate-during-build -S"C:\Users\source\ai engine\engine" -B"C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio""
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build InferenceEngine/rebuild_cache: phony InferenceEngine/CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build InferenceEngine/list_install_components: phony


#############################################
# Utility command for install

build InferenceEngine/CMakeFiles/install.util: CUSTOM_COMMAND InferenceEngine/all
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -P cmake_install.cmake"
  DESC = Install the project...
  pool = console
  restat = 1

build InferenceEngine/install: phony InferenceEngine/CMakeFiles/install.util


#############################################
# Utility command for install/local

build InferenceEngine/CMakeFiles/install/local.util: CUSTOM_COMMAND InferenceEngine/all
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake"
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build InferenceEngine/install/local: phony InferenceEngine/CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build InferenceEngine/CMakeFiles/install/strip.util: CUSTOM_COMMAND InferenceEngine/all
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake"
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build InferenceEngine/install/strip: phony InferenceEngine/CMakeFiles/install/strip.util

# =============================================================================
# Write statements declared in CMakeLists.txt:
# C:/Users/source/ai engine/engine/InferenceEngine/CMakeLists.txt
# =============================================================================


#############################################
# Utility command for edit_cache

build InferenceEngine/llama.cpp/CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -E echo "No interactive CMake dialog available.""
  DESC = No interactive CMake dialog available...
  restat = 1

build InferenceEngine/llama.cpp/edit_cache: phony InferenceEngine/llama.cpp/CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build InferenceEngine/llama.cpp/CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" --regenerate-during-build -S"C:\Users\source\ai engine\engine" -B"C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio""
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build InferenceEngine/llama.cpp/rebuild_cache: phony InferenceEngine/llama.cpp/CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build InferenceEngine/llama.cpp/list_install_components: phony


#############################################
# Utility command for install

build InferenceEngine/llama.cpp/CMakeFiles/install.util: CUSTOM_COMMAND InferenceEngine/llama.cpp/all
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -P cmake_install.cmake"
  DESC = Install the project...
  pool = console
  restat = 1

build InferenceEngine/llama.cpp/install: phony InferenceEngine/llama.cpp/CMakeFiles/install.util


#############################################
# Utility command for install/local

build InferenceEngine/llama.cpp/CMakeFiles/install/local.util: CUSTOM_COMMAND InferenceEngine/llama.cpp/all
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake"
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build InferenceEngine/llama.cpp/install/local: phony InferenceEngine/llama.cpp/CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build InferenceEngine/llama.cpp/CMakeFiles/install/strip.util: CUSTOM_COMMAND InferenceEngine/llama.cpp/all
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake"
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build InferenceEngine/llama.cpp/install/strip: phony InferenceEngine/llama.cpp/CMakeFiles/install/strip.util

# =============================================================================
# Write statements declared in CMakeLists.txt:
# C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/CMakeLists.txt
# =============================================================================


#############################################
# Utility command for edit_cache

build InferenceEngine/llama.cpp/ggml/CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\ggml" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -E echo "No interactive CMake dialog available.""
  DESC = No interactive CMake dialog available...
  restat = 1

build InferenceEngine/llama.cpp/ggml/edit_cache: phony InferenceEngine/llama.cpp/ggml/CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build InferenceEngine/llama.cpp/ggml/CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\ggml" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" --regenerate-during-build -S"C:\Users\source\ai engine\engine" -B"C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio""
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build InferenceEngine/llama.cpp/ggml/rebuild_cache: phony InferenceEngine/llama.cpp/ggml/CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build InferenceEngine/llama.cpp/ggml/list_install_components: phony


#############################################
# Utility command for install

build InferenceEngine/llama.cpp/ggml/CMakeFiles/install.util: CUSTOM_COMMAND InferenceEngine/llama.cpp/ggml/all
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\ggml" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -P cmake_install.cmake"
  DESC = Install the project...
  pool = console
  restat = 1

build InferenceEngine/llama.cpp/ggml/install: phony InferenceEngine/llama.cpp/ggml/CMakeFiles/install.util


#############################################
# Utility command for install/local

build InferenceEngine/llama.cpp/ggml/CMakeFiles/install/local.util: CUSTOM_COMMAND InferenceEngine/llama.cpp/ggml/all
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\ggml" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake"
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build InferenceEngine/llama.cpp/ggml/install/local: phony InferenceEngine/llama.cpp/ggml/CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build InferenceEngine/llama.cpp/ggml/CMakeFiles/install/strip.util: CUSTOM_COMMAND InferenceEngine/llama.cpp/ggml/all
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\ggml" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake"
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build InferenceEngine/llama.cpp/ggml/install/strip: phony InferenceEngine/llama.cpp/ggml/CMakeFiles/install/strip.util

# =============================================================================
# Write statements declared in CMakeLists.txt:
# C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/CMakeLists.txt
# =============================================================================

# =============================================================================
# Object build statements for SHARED_LIBRARY target ggml


#############################################
# Order-only phony target for ggml

build cmake_object_order_depends_target_ggml: phony || .

build InferenceEngine/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml.c.obj: C_COMPILER__ggml_unscanned_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/ggml/src/ggml.c || cmake_object_order_depends_target_ggml
  DEFINES = -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_CRT_SECURE_NO_WARNINGS -D_XOPEN_SOURCE=600 -Dggml_EXPORTS
  DEP_FILE = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir\ggml.c.obj.d
  FLAGS = -O0 -std=gnu11 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -march=native -fopenmp=libomp
  INCLUDES = -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/../include" -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/."
  OBJECT_DIR = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir
  OBJECT_FILE_DIR = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir\
  TARGET_PDB = bin\ggml.pdb

build InferenceEngine/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-alloc.c.obj: C_COMPILER__ggml_unscanned_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/ggml/src/ggml-alloc.c || cmake_object_order_depends_target_ggml
  DEFINES = -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_CRT_SECURE_NO_WARNINGS -D_XOPEN_SOURCE=600 -Dggml_EXPORTS
  DEP_FILE = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir\ggml-alloc.c.obj.d
  FLAGS = -O0 -std=gnu11 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -march=native -fopenmp=libomp
  INCLUDES = -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/../include" -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/."
  OBJECT_DIR = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir
  OBJECT_FILE_DIR = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir\
  TARGET_PDB = bin\ggml.pdb

build InferenceEngine/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend.c.obj: C_COMPILER__ggml_unscanned_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/ggml/src/ggml-backend.c || cmake_object_order_depends_target_ggml
  DEFINES = -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_CRT_SECURE_NO_WARNINGS -D_XOPEN_SOURCE=600 -Dggml_EXPORTS
  DEP_FILE = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir\ggml-backend.c.obj.d
  FLAGS = -O0 -std=gnu11 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -march=native -fopenmp=libomp
  INCLUDES = -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/../include" -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/."
  OBJECT_DIR = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir
  OBJECT_FILE_DIR = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir\
  TARGET_PDB = bin\ggml.pdb

build InferenceEngine/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.obj: C_COMPILER__ggml_unscanned_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/ggml/src/ggml-quants.c || cmake_object_order_depends_target_ggml
  DEFINES = -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_CRT_SECURE_NO_WARNINGS -D_XOPEN_SOURCE=600 -Dggml_EXPORTS
  DEP_FILE = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir\ggml-quants.c.obj.d
  FLAGS = -O0 -std=gnu11 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -march=native -fopenmp=libomp
  INCLUDES = -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/../include" -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/."
  OBJECT_DIR = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir
  OBJECT_FILE_DIR = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir\
  TARGET_PDB = bin\ggml.pdb

build InferenceEngine/llama.cpp/ggml/src/CMakeFiles/ggml.dir/llamafile/sgemm.cpp.obj: CXX_COMPILER__ggml_unscanned_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/ggml/src/llamafile/sgemm.cpp || cmake_object_order_depends_target_ggml
  DEFINES = -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_CRT_SECURE_NO_WARNINGS -D_XOPEN_SOURCE=600 -Dggml_EXPORTS
  DEP_FILE = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir\llamafile\sgemm.cpp.obj.d
  FLAGS = -O0 -std=gnu++14 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -march=native -fopenmp=libomp
  INCLUDES = -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/../include" -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/."
  OBJECT_DIR = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir
  OBJECT_FILE_DIR = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir\llamafile
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir\
  TARGET_PDB = bin\ggml.pdb

build InferenceEngine/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-aarch64.c.obj: C_COMPILER__ggml_unscanned_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/ggml/src/ggml-aarch64.c || cmake_object_order_depends_target_ggml
  DEFINES = -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_CRT_SECURE_NO_WARNINGS -D_XOPEN_SOURCE=600 -Dggml_EXPORTS
  DEP_FILE = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir\ggml-aarch64.c.obj.d
  FLAGS = -O0 -std=gnu11 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -march=native -fopenmp=libomp
  INCLUDES = -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/../include" -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/."
  OBJECT_DIR = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir
  OBJECT_FILE_DIR = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir\
  TARGET_PDB = bin\ggml.pdb


# =============================================================================
# Link build statements for SHARED_LIBRARY target ggml


#############################################
# Link the shared library bin\ggml.dll

build bin/ggml.dll InferenceEngine/llama.cpp/ggml/src/ggml.lib: CXX_SHARED_LIBRARY_LINKER__ggml_Debug InferenceEngine/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml.c.obj InferenceEngine/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-alloc.c.obj InferenceEngine/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend.c.obj InferenceEngine/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.obj InferenceEngine/llama.cpp/ggml/src/CMakeFiles/ggml.dir/llamafile/sgemm.cpp.obj InferenceEngine/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-aarch64.c.obj | C$:/Program$ Files/Microsoft$ Visual$ Studio/2022/Community/VC/Tools/MSVC/14.43.34808/lib/x64/libomp.lib
  LANGUAGE_COMPILE_FLAGS = -O0 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview
  LINK_FLAGS = -fuse-ld=lld-link -Xlinker /DEF:InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir\.\exports.def
  LINK_LIBRARIES = "C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.43.34808/lib/x64/libomp.lib"  -lkernel32 -luser32 -lgdi32 -lwinspool -lshell32 -lole32 -loleaut32 -luuid -lcomdlg32 -ladvapi32 -loldnames
  OBJECT_DIR = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir
  POST_BUILD = cd .
  PRE_LINK = C:\WINDOWS\system32\cmd.exe /C ""C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -E __create_def "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir\.\exports.def" "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir\.\exports.def.objs" --nm="C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\Llvm\x64\bin\llvm-nm.exe" && cd "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio""
  RESTAT = 1
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\ggml\src\CMakeFiles\ggml.dir\
  TARGET_FILE = bin\ggml.dll
  TARGET_IMPLIB = InferenceEngine\llama.cpp\ggml\src\ggml.lib
  TARGET_PDB = bin\ggml.pdb


#############################################
# Utility command for edit_cache

build InferenceEngine/llama.cpp/ggml/src/CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\ggml\src" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -E echo "No interactive CMake dialog available.""
  DESC = No interactive CMake dialog available...
  restat = 1

build InferenceEngine/llama.cpp/ggml/src/edit_cache: phony InferenceEngine/llama.cpp/ggml/src/CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build InferenceEngine/llama.cpp/ggml/src/CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\ggml\src" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" --regenerate-during-build -S"C:\Users\source\ai engine\engine" -B"C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio""
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build InferenceEngine/llama.cpp/ggml/src/rebuild_cache: phony InferenceEngine/llama.cpp/ggml/src/CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build InferenceEngine/llama.cpp/ggml/src/list_install_components: phony


#############################################
# Utility command for install

build InferenceEngine/llama.cpp/ggml/src/CMakeFiles/install.util: CUSTOM_COMMAND InferenceEngine/llama.cpp/ggml/src/all
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\ggml\src" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -P cmake_install.cmake"
  DESC = Install the project...
  pool = console
  restat = 1

build InferenceEngine/llama.cpp/ggml/src/install: phony InferenceEngine/llama.cpp/ggml/src/CMakeFiles/install.util


#############################################
# Utility command for install/local

build InferenceEngine/llama.cpp/ggml/src/CMakeFiles/install/local.util: CUSTOM_COMMAND InferenceEngine/llama.cpp/ggml/src/all
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\ggml\src" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake"
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build InferenceEngine/llama.cpp/ggml/src/install/local: phony InferenceEngine/llama.cpp/ggml/src/CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build InferenceEngine/llama.cpp/ggml/src/CMakeFiles/install/strip.util: CUSTOM_COMMAND InferenceEngine/llama.cpp/ggml/src/all
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\ggml\src" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake"
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build InferenceEngine/llama.cpp/ggml/src/install/strip: phony InferenceEngine/llama.cpp/ggml/src/CMakeFiles/install/strip.util

# =============================================================================
# Write statements declared in CMakeLists.txt:
# C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/CMakeLists.txt
# =============================================================================

# =============================================================================
# Object build statements for SHARED_LIBRARY target llama


#############################################
# Order-only phony target for llama

build cmake_object_order_depends_target_llama: phony || cmake_object_order_depends_target_ggml

build InferenceEngine/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.obj: CXX_COMPILER__llama_unscanned_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/src/llama.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DLLAMA_BUILD -DLLAMA_SHARED -D_CRT_SECURE_NO_WARNINGS -Dllama_EXPORTS
  DEP_FILE = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir\llama.cpp.obj.d
  FLAGS = -O0 -std=gnu++20 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview
  INCLUDES = -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/." -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/../include" -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/../include"
  OBJECT_DIR = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir
  OBJECT_FILE_DIR = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir\
  TARGET_PDB = bin\llama.pdb

build InferenceEngine/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.obj: CXX_COMPILER__llama_unscanned_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/src/llama-vocab.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DLLAMA_BUILD -DLLAMA_SHARED -D_CRT_SECURE_NO_WARNINGS -Dllama_EXPORTS
  DEP_FILE = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir\llama-vocab.cpp.obj.d
  FLAGS = -O0 -std=gnu++20 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview
  INCLUDES = -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/." -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/../include" -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/../include"
  OBJECT_DIR = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir
  OBJECT_FILE_DIR = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir\
  TARGET_PDB = bin\llama.pdb

build InferenceEngine/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.obj: CXX_COMPILER__llama_unscanned_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/src/llama-grammar.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DLLAMA_BUILD -DLLAMA_SHARED -D_CRT_SECURE_NO_WARNINGS -Dllama_EXPORTS
  DEP_FILE = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir\llama-grammar.cpp.obj.d
  FLAGS = -O0 -std=gnu++20 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview
  INCLUDES = -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/." -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/../include" -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/../include"
  OBJECT_DIR = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir
  OBJECT_FILE_DIR = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir\
  TARGET_PDB = bin\llama.pdb

build InferenceEngine/llama.cpp/src/CMakeFiles/llama.dir/llama-sampling.cpp.obj: CXX_COMPILER__llama_unscanned_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/src/llama-sampling.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DLLAMA_BUILD -DLLAMA_SHARED -D_CRT_SECURE_NO_WARNINGS -Dllama_EXPORTS
  DEP_FILE = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir\llama-sampling.cpp.obj.d
  FLAGS = -O0 -std=gnu++20 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview
  INCLUDES = -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/." -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/../include" -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/../include"
  OBJECT_DIR = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir
  OBJECT_FILE_DIR = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir\
  TARGET_PDB = bin\llama.pdb

build InferenceEngine/llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.obj: CXX_COMPILER__llama_unscanned_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/src/unicode.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DLLAMA_BUILD -DLLAMA_SHARED -D_CRT_SECURE_NO_WARNINGS -Dllama_EXPORTS
  DEP_FILE = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir\unicode.cpp.obj.d
  FLAGS = -O0 -std=gnu++20 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview
  INCLUDES = -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/." -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/../include" -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/../include"
  OBJECT_DIR = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir
  OBJECT_FILE_DIR = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir\
  TARGET_PDB = bin\llama.pdb

build InferenceEngine/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.obj: CXX_COMPILER__llama_unscanned_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/src/unicode-data.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DLLAMA_BUILD -DLLAMA_SHARED -D_CRT_SECURE_NO_WARNINGS -Dllama_EXPORTS
  DEP_FILE = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir\unicode-data.cpp.obj.d
  FLAGS = -O0 -std=gnu++20 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview
  INCLUDES = -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/." -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/../include" -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/../include"
  OBJECT_DIR = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir
  OBJECT_FILE_DIR = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir\
  TARGET_PDB = bin\llama.pdb


# =============================================================================
# Link build statements for SHARED_LIBRARY target llama


#############################################
# Link the shared library bin\llama.dll

build bin/llama.dll InferenceEngine/llama.cpp/src/llama.lib: CXX_SHARED_LIBRARY_LINKER__llama_Debug InferenceEngine/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.obj InferenceEngine/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.obj InferenceEngine/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.obj InferenceEngine/llama.cpp/src/CMakeFiles/llama.dir/llama-sampling.cpp.obj InferenceEngine/llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.obj InferenceEngine/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.obj | InferenceEngine/llama.cpp/ggml/src/ggml.lib || bin/ggml.dll
  LANGUAGE_COMPILE_FLAGS = -O0 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview
  LINK_FLAGS = -fuse-ld=lld-link -Xlinker /DEF:InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir\.\exports.def
  LINK_LIBRARIES = InferenceEngine/llama.cpp/ggml/src/ggml.lib  -lkernel32 -luser32 -lgdi32 -lwinspool -lshell32 -lole32 -loleaut32 -luuid -lcomdlg32 -ladvapi32 -loldnames
  OBJECT_DIR = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir
  POST_BUILD = cd .
  PRE_LINK = C:\WINDOWS\system32\cmd.exe /C ""C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -E __create_def "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir\.\exports.def" "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir\.\exports.def.objs" --nm="C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\Llvm\x64\bin\llvm-nm.exe" && cd "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio""
  RESTAT = 1
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\src\CMakeFiles\llama.dir\
  TARGET_FILE = bin\llama.dll
  TARGET_IMPLIB = InferenceEngine\llama.cpp\src\llama.lib
  TARGET_PDB = bin\llama.pdb


#############################################
# Utility command for edit_cache

build InferenceEngine/llama.cpp/src/CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\src" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -E echo "No interactive CMake dialog available.""
  DESC = No interactive CMake dialog available...
  restat = 1

build InferenceEngine/llama.cpp/src/edit_cache: phony InferenceEngine/llama.cpp/src/CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build InferenceEngine/llama.cpp/src/CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\src" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" --regenerate-during-build -S"C:\Users\source\ai engine\engine" -B"C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio""
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build InferenceEngine/llama.cpp/src/rebuild_cache: phony InferenceEngine/llama.cpp/src/CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build InferenceEngine/llama.cpp/src/list_install_components: phony


#############################################
# Utility command for install

build InferenceEngine/llama.cpp/src/CMakeFiles/install.util: CUSTOM_COMMAND InferenceEngine/llama.cpp/src/all
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\src" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -P cmake_install.cmake"
  DESC = Install the project...
  pool = console
  restat = 1

build InferenceEngine/llama.cpp/src/install: phony InferenceEngine/llama.cpp/src/CMakeFiles/install.util


#############################################
# Utility command for install/local

build InferenceEngine/llama.cpp/src/CMakeFiles/install/local.util: CUSTOM_COMMAND InferenceEngine/llama.cpp/src/all
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\src" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake"
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build InferenceEngine/llama.cpp/src/install/local: phony InferenceEngine/llama.cpp/src/CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build InferenceEngine/llama.cpp/src/CMakeFiles/install/strip.util: CUSTOM_COMMAND InferenceEngine/llama.cpp/src/all
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\src" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake"
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build InferenceEngine/llama.cpp/src/install/strip: phony InferenceEngine/llama.cpp/src/CMakeFiles/install/strip.util

# =============================================================================
# Write statements declared in CMakeLists.txt:
# C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/CMakeLists.txt
# =============================================================================

# =============================================================================
# Object build statements for OBJECT_LIBRARY target build_info


#############################################
# Order-only phony target for build_info

build cmake_object_order_depends_target_build_info: phony || C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/common/build-info.cpp

build InferenceEngine/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.obj: CXX_COMPILER__build_info_unscanned_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/common/build-info.cpp || cmake_object_order_depends_target_build_info
  DEFINES = -D_CRT_SECURE_NO_WARNINGS
  DEP_FILE = InferenceEngine\llama.cpp\common\CMakeFiles\build_info.dir\build-info.cpp.obj.d
  FLAGS = -O0 -std=gnu++20 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview
  OBJECT_DIR = InferenceEngine\llama.cpp\common\CMakeFiles\build_info.dir
  OBJECT_FILE_DIR = InferenceEngine\llama.cpp\common\CMakeFiles\build_info.dir
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\common\CMakeFiles\build_info.dir\
  TARGET_PDB = ""



#############################################
# Object library build_info

build InferenceEngine/llama.cpp/common/build_info: phony InferenceEngine/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.obj

# =============================================================================
# Object build statements for STATIC_LIBRARY target common


#############################################
# Order-only phony target for common

build cmake_object_order_depends_target_common: phony || cmake_object_order_depends_target_build_info cmake_object_order_depends_target_ggml cmake_object_order_depends_target_llama

build InferenceEngine/llama.cpp/common/CMakeFiles/common.dir/common.cpp.obj: CXX_COMPILER__common_unscanned_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/common/common.cpp || cmake_object_order_depends_target_common
  DEFINES = -D_CRT_SECURE_NO_WARNINGS
  DEP_FILE = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir\common.cpp.obj.d
  FLAGS = -O0 -std=gnu++20 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview
  INCLUDES = -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/common/." -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/." -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/../include" -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/../include"
  OBJECT_DIR = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir
  OBJECT_FILE_DIR = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir\common.pdb
  TARGET_PDB = InferenceEngine\llama.cpp\common\common.pdb

build InferenceEngine/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.obj: CXX_COMPILER__common_unscanned_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/common/sampling.cpp || cmake_object_order_depends_target_common
  DEFINES = -D_CRT_SECURE_NO_WARNINGS
  DEP_FILE = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir\sampling.cpp.obj.d
  FLAGS = -O0 -std=gnu++20 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview
  INCLUDES = -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/common/." -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/." -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/../include" -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/../include"
  OBJECT_DIR = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir
  OBJECT_FILE_DIR = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir\common.pdb
  TARGET_PDB = InferenceEngine\llama.cpp\common\common.pdb

build InferenceEngine/llama.cpp/common/CMakeFiles/common.dir/console.cpp.obj: CXX_COMPILER__common_unscanned_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/common/console.cpp || cmake_object_order_depends_target_common
  DEFINES = -D_CRT_SECURE_NO_WARNINGS
  DEP_FILE = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir\console.cpp.obj.d
  FLAGS = -O0 -std=gnu++20 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview
  INCLUDES = -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/common/." -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/." -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/../include" -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/../include"
  OBJECT_DIR = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir
  OBJECT_FILE_DIR = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir\common.pdb
  TARGET_PDB = InferenceEngine\llama.cpp\common\common.pdb

build InferenceEngine/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.obj: CXX_COMPILER__common_unscanned_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/common/grammar-parser.cpp || cmake_object_order_depends_target_common
  DEFINES = -D_CRT_SECURE_NO_WARNINGS
  DEP_FILE = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir\grammar-parser.cpp.obj.d
  FLAGS = -O0 -std=gnu++20 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview
  INCLUDES = -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/common/." -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/." -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/../include" -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/../include"
  OBJECT_DIR = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir
  OBJECT_FILE_DIR = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir\common.pdb
  TARGET_PDB = InferenceEngine\llama.cpp\common\common.pdb

build InferenceEngine/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.obj: CXX_COMPILER__common_unscanned_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/common/json-schema-to-grammar.cpp || cmake_object_order_depends_target_common
  DEFINES = -D_CRT_SECURE_NO_WARNINGS
  DEP_FILE = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir\json-schema-to-grammar.cpp.obj.d
  FLAGS = -O0 -std=gnu++20 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview
  INCLUDES = -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/common/." -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/." -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/../include" -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/../include"
  OBJECT_DIR = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir
  OBJECT_FILE_DIR = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir\common.pdb
  TARGET_PDB = InferenceEngine\llama.cpp\common\common.pdb

build InferenceEngine/llama.cpp/common/CMakeFiles/common.dir/train.cpp.obj: CXX_COMPILER__common_unscanned_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/common/train.cpp || cmake_object_order_depends_target_common
  DEFINES = -D_CRT_SECURE_NO_WARNINGS
  DEP_FILE = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir\train.cpp.obj.d
  FLAGS = -O0 -std=gnu++20 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview
  INCLUDES = -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/common/." -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/." -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/../include" -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/../include"
  OBJECT_DIR = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir
  OBJECT_FILE_DIR = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir\common.pdb
  TARGET_PDB = InferenceEngine\llama.cpp\common\common.pdb

build InferenceEngine/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.obj: CXX_COMPILER__common_unscanned_Debug C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/common/ngram-cache.cpp || cmake_object_order_depends_target_common
  DEFINES = -D_CRT_SECURE_NO_WARNINGS
  DEP_FILE = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir\ngram-cache.cpp.obj.d
  FLAGS = -O0 -std=gnu++20 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview
  INCLUDES = -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/common/." -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/." -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/src/../include" -I"C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/ggml/src/../include"
  OBJECT_DIR = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir
  OBJECT_FILE_DIR = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir\common.pdb
  TARGET_PDB = InferenceEngine\llama.cpp\common\common.pdb


# =============================================================================
# Link build statements for STATIC_LIBRARY target common


#############################################
# Link the static library InferenceEngine\llama.cpp\common\common.lib

build InferenceEngine/llama.cpp/common/common.lib: CXX_STATIC_LIBRARY_LINKER__common_Debug InferenceEngine/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.obj InferenceEngine/llama.cpp/common/CMakeFiles/common.dir/common.cpp.obj InferenceEngine/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.obj InferenceEngine/llama.cpp/common/CMakeFiles/common.dir/console.cpp.obj InferenceEngine/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.obj InferenceEngine/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.obj InferenceEngine/llama.cpp/common/CMakeFiles/common.dir/train.cpp.obj InferenceEngine/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.obj || InferenceEngine/llama.cpp/common/build_info bin/ggml.dll bin/llama.dll
  LANGUAGE_COMPILE_FLAGS = -O0 -D_DEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrtd -g -Xclang -gcodeview
  OBJECT_DIR = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir
  POST_BUILD = cd .
  PRE_LINK = cd .
  TARGET_COMPILE_PDB = InferenceEngine\llama.cpp\common\CMakeFiles\common.dir\common.pdb
  TARGET_FILE = InferenceEngine\llama.cpp\common\common.lib
  TARGET_PDB = InferenceEngine\llama.cpp\common\common.pdb


#############################################
# Utility command for edit_cache

build InferenceEngine/llama.cpp/common/CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\common" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -E echo "No interactive CMake dialog available.""
  DESC = No interactive CMake dialog available...
  restat = 1

build InferenceEngine/llama.cpp/common/edit_cache: phony InferenceEngine/llama.cpp/common/CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build InferenceEngine/llama.cpp/common/CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\common" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" --regenerate-during-build -S"C:\Users\source\ai engine\engine" -B"C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio""
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build InferenceEngine/llama.cpp/common/rebuild_cache: phony InferenceEngine/llama.cpp/common/CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build InferenceEngine/llama.cpp/common/list_install_components: phony


#############################################
# Utility command for install

build InferenceEngine/llama.cpp/common/CMakeFiles/install.util: CUSTOM_COMMAND InferenceEngine/llama.cpp/common/all
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\common" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -P cmake_install.cmake"
  DESC = Install the project...
  pool = console
  restat = 1

build InferenceEngine/llama.cpp/common/install: phony InferenceEngine/llama.cpp/common/CMakeFiles/install.util


#############################################
# Utility command for install/local

build InferenceEngine/llama.cpp/common/CMakeFiles/install/local.util: CUSTOM_COMMAND InferenceEngine/llama.cpp/common/all
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\common" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake"
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build InferenceEngine/llama.cpp/common/install/local: phony InferenceEngine/llama.cpp/common/CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build InferenceEngine/llama.cpp/common/CMakeFiles/install/strip.util: CUSTOM_COMMAND InferenceEngine/llama.cpp/common/all
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\cmake-build-debug-visual-studio\InferenceEngine\llama.cpp\common" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake"
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build InferenceEngine/llama.cpp/common/install/strip: phony InferenceEngine/llama.cpp/common/CMakeFiles/install/strip.util


#############################################
# Custom command for C:\Users\source\ai engine\engine\InferenceEngine\llama.cpp\common\build-info.cpp

build C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/common/build-info.cpp: CUSTOM_COMMAND C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/common/build-info.cpp.in
  COMMAND = C:\WINDOWS\system32\cmd.exe /C "cd /D "C:\Users\source\ai engine\engine\InferenceEngine\llama.cpp" && "C:\Program Files\JetBrains\CLion 2024.3.4\bin\cmake\win\x64\bin\cmake.exe" -DMSVC= -DCMAKE_C_COMPILER_VERSION=19.1.1 -DCMAKE_C_COMPILER_ID=Clang -DCMAKE_VS_PLATFORM_NAME= "-DCMAKE_C_COMPILER=C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/Llvm/x64/bin/clang.exe" -P "C:/Users/source/ai engine/engine/InferenceEngine/llama.cpp/common/cmake/build-info-gen-cpp.cmake""
  DESC = Generating build details from Git
  restat = 1

# =============================================================================
# Target aliases.

build InferenceEngine: phony InferenceEngine/InferenceEngine.lib

build InferenceEngine.lib: phony InferenceEngine/InferenceEngine.lib

build build_info: phony InferenceEngine/llama.cpp/common/build_info

build common: phony InferenceEngine/llama.cpp/common/common.lib

build common.lib: phony InferenceEngine/llama.cpp/common/common.lib

build ggml: phony bin/ggml.dll

build ggml.dll: phony bin/ggml.dll

build llama: phony bin/llama.dll

build llama.dll: phony bin/llama.dll

# =============================================================================
# Folder targets.

# =============================================================================

#############################################
# Folder: C:/Users/source/ai engine/engine/cmake-build-debug-visual-studio

build all: phony InferenceEngine/all

# =============================================================================

#############################################
# Folder: C:/Users/source/ai engine/engine/cmake-build-debug-visual-studio/InferenceEngine

build InferenceEngine/all: phony InferenceEngine/InferenceEngine.lib InferenceEngine/llama.cpp/all

# =============================================================================

#############################################
# Folder: C:/Users/source/ai engine/engine/cmake-build-debug-visual-studio/InferenceEngine/llama.cpp

build InferenceEngine/llama.cpp/all: phony InferenceEngine/llama.cpp/ggml/all InferenceEngine/llama.cpp/src/all InferenceEngine/llama.cpp/common/all

# =============================================================================

#############################################
# Folder: C:/Users/source/ai engine/engine/cmake-build-debug-visual-studio/InferenceEngine/llama.cpp/common

build InferenceEngine/llama.cpp/common/all: phony InferenceEngine/llama.cpp/common/build_info InferenceEngine/llama.cpp/common/common.lib

# =============================================================================

#############################################
# Folder: C:/Users/source/ai engine/engine/cmake-build-debug-visual-studio/InferenceEngine/llama.cpp/ggml

build InferenceEngine/llama.cpp/ggml/all: phony InferenceEngine/llama.cpp/ggml/src/all

# =============================================================================

#############################################
# Folder: C:/Users/source/ai engine/engine/cmake-build-debug-visual-studio/InferenceEngine/llama.cpp/ggml/src

build InferenceEngine/llama.cpp/ggml/src/all: phony bin/ggml.dll

# =============================================================================

#############################################
# Folder: C:/Users/source/ai engine/engine/cmake-build-debug-visual-studio/InferenceEngine/llama.cpp/src

build InferenceEngine/llama.cpp/src/all: phony bin/llama.dll

# =============================================================================
# Built-in targets


#############################################
# Re-run CMake if any of its inputs changed.

build build.ninja: RERUN_CMAKE | C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/BasicConfigVersion-SameMajorVersion.cmake.in C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeCInformation.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeCXXInformation.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeCheckCompilerFlagCommonPatterns.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeCommonLanguageInclude.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeGenericSystem.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeInitializeConfigs.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeLanguageInformation.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakePackageConfigHelpers.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeParseImplicitLinkInfo.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeRCInformation.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeSystemSpecificInformation.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeSystemSpecificInitialize.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CheckCSourceCompiles.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CheckCXXCompilerFlag.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CheckCXXSourceCompiles.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CheckIncludeFile.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CheckIncludeFileCXX.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CheckLibraryExists.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Compiler/CMakeCommonCompilerMacros.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Compiler/Clang-C.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Compiler/Clang-CXX.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Compiler/Clang.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/FindGit.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/FindOpenMP.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/FindPackageHandleStandardArgs.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/FindPackageMessage.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/FindThreads.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/GNUInstallDirs.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Internal/CheckCompilerFlag.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Internal/CheckFlagCommonConfig.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Internal/CheckSourceCompiles.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Platform/Windows-Clang-C.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Platform/Windows-Clang-CXX.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Platform/Windows-Clang.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Platform/Windows-Initialize.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Platform/Windows.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Platform/WindowsPaths.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/WriteBasicConfigVersionFile.cmake C$:/Users/source/ai$ engine/engine/CMakeLists.txt C$:/Users/source/ai$ engine/engine/InferenceEngine/CMakeLists.txt C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/CMakeLists.txt C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/cmake/build-info.cmake C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/cmake/llama-config.cmake.in C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/cmake/llama.pc.in C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/common/CMakeLists.txt C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/ggml/CMakeLists.txt C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/ggml/src/CMakeLists.txt C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/src/CMakeLists.txt CMakeCache.txt CMakeFiles/3.30.5/CMakeCCompiler.cmake CMakeFiles/3.30.5/CMakeCXXCompiler.cmake CMakeFiles/3.30.5/CMakeRCCompiler.cmake CMakeFiles/3.30.5/CMakeSystem.cmake
  pool = console


#############################################
# A missing CMake input file is not an error.

build C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/BasicConfigVersion-SameMajorVersion.cmake.in C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeCInformation.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeCXXInformation.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeCheckCompilerFlagCommonPatterns.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeCommonLanguageInclude.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeGenericSystem.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeInitializeConfigs.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeLanguageInformation.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakePackageConfigHelpers.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeParseImplicitLinkInfo.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeRCInformation.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeSystemSpecificInformation.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CMakeSystemSpecificInitialize.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CheckCSourceCompiles.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CheckCXXCompilerFlag.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CheckCXXSourceCompiles.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CheckIncludeFile.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CheckIncludeFileCXX.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/CheckLibraryExists.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Compiler/CMakeCommonCompilerMacros.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Compiler/Clang-C.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Compiler/Clang-CXX.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Compiler/Clang.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/FindGit.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/FindOpenMP.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/FindPackageHandleStandardArgs.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/FindPackageMessage.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/FindThreads.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/GNUInstallDirs.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Internal/CheckCompilerFlag.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Internal/CheckFlagCommonConfig.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Internal/CheckSourceCompiles.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Platform/Windows-Clang-C.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Platform/Windows-Clang-CXX.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Platform/Windows-Clang.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Platform/Windows-Initialize.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Platform/Windows.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/Platform/WindowsPaths.cmake C$:/Program$ Files/JetBrains/CLion$ 2024.3.4/bin/cmake/win/x64/share/cmake-3.30/Modules/WriteBasicConfigVersionFile.cmake C$:/Users/source/ai$ engine/engine/CMakeLists.txt C$:/Users/source/ai$ engine/engine/InferenceEngine/CMakeLists.txt C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/CMakeLists.txt C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/cmake/build-info.cmake C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/cmake/llama-config.cmake.in C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/cmake/llama.pc.in C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/common/CMakeLists.txt C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/ggml/CMakeLists.txt C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/ggml/src/CMakeLists.txt C$:/Users/source/ai$ engine/engine/InferenceEngine/llama.cpp/src/CMakeLists.txt CMakeCache.txt CMakeFiles/3.30.5/CMakeCCompiler.cmake CMakeFiles/3.30.5/CMakeCXXCompiler.cmake CMakeFiles/3.30.5/CMakeRCCompiler.cmake CMakeFiles/3.30.5/CMakeSystem.cmake: phony


#############################################
# Clean all the built files.

build clean: CLEAN


#############################################
# Print all primary targets available.

build help: HELP


#############################################
# Make the all target the default.

default all
